{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055bba10-3d07-4466-9ca2-72cf054f1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3086aa1a-cc99-4163-bcc4-0c647f436982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and Prepare Dataset\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx].reshape(28, 28).astype('uint8')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load CSV\n",
    "data_path = 'fashion-mnist_train.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4204e5a4-5511-409f-b88d-10bd2369df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # ResNet expects 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FashionMNISTDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = FashionMNISTDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = FashionMNISTDataset(X_test, y_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47452754-d227-4ad7-921e-ae471c7b596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meer\n"
     ]
    }
   ],
   "source": [
    "# again for 10 batch size due to computation limitation Create dataloaders\n",
    "print(\"meer\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c0c104-33f3-46ad-8117-55745c5be4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PMLS\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load Pre-trained Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Adjust for single-channel input\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)  # Fashion MNIST has 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadf6a81-8083-48d8-91dc-f26f3bc8e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e26ef007-a1ab-44c4-bcd1-f3529a47ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067eba3a-d2a7-40a6-b281-d598d81bca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loops\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = 100.0 * correct / total\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        val_acc, val_loss = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return train_acc_history, val_acc_history, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371e2265-498a-42c9-a0aa-7acdbc766136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100.0 * correct / total\n",
    "    return val_acc, val_loss / len(val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424c92a-3fca-4fc1-bacf-5836e45413ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model Training use 10 epochs because of computation power \n",
    "train_acc_history, val_acc_history, train_loss_history, val_loss_history = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847205fa-7de4-4832-8f5e-359a030ff0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation on Test Set\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_acc = 100.0 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    return all_labels, all_preds\n",
    "\n",
    "all_labels, all_preds = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68503eb-25d7-487e-aa8f-541f9457fb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698db72-31cc-4101-afdb-2aae070382c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Training Curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# for building Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13e01f24-59a7-4b5b-bc98-763d0d83771a",
   "metadata": {},
   "source": [
    "Number of Classes: 10 T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "Number of Images: 60,000 training images\n",
    "Image Dimensions: 28x28 grayscale images\n",
    "Source: Fashion MNIST by Zalando Research\n",
    "Description: Fashion MNIST is a drop-in replacement for the original MNIST dataset. It is used as a benchmark for image classification tasks, containing images of clothing items instead of handwritten digits.\n",
    "\n",
    "\n",
    "Model Selected: ResNet18 (Residual Neural Network)\n",
    "Reason for Selection:\n",
    "\n",
    "Transfer Learning: ResNet18 is a widely used pre-trained model trained on ImageNet. Its architecture is designed to handle complex feature extraction with residual connections to avoid vanishing gradients.\n",
    "Simplicity and Effectiveness: ResNet18 is relatively lightweight compared to deeper models like ResNet50 or VGG but still provides excellent performance on small to medium-scale datasets.\n",
    "Flexibility: The first convolutional layer and fully connected layer were easily adapted to support Fashion MNIST's grayscale images and 10 output classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
